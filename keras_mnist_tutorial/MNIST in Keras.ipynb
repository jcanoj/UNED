{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcanoj/UNED/blob/master/keras_mnist_tutorial/MNIST%20in%20Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extraido de [github-AviatorMoser: keras-mnist-tutorial](https://github.com/AviatorMoser/keras-mnist-tutorial)"
      ],
      "metadata": {
        "id": "EvJRY6-yElY1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "collapsed": true,
        "id": "9eLTHsB6EkTi"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60titYcjEkTl"
      },
      "source": [
        "# Introduction to Deep Learning with Keras and TensorFlow\n",
        "\n",
        "**Daniel Moser (UT Southwestern Medical Center)**\n",
        "\n",
        "**Resources: [Xavier Snelgrove](https://github.com/wxs/keras-mnist-tutorial), [Yash Katariya](https://github.com/yashk2810/MNIST-Keras)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiSXoXhkEkTn"
      },
      "source": [
        "To help you understand the fundamentals of deep learning, this demo will walk through the basic steps of building two toy models for classifying handwritten numbers with accuracies surpassing 95%. The first model will be a basic fully-connected neural network, and the second model will be a deeper network that introduces the concepts of convolution and pooling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NmgRiHHEkTp"
      },
      "source": [
        "## The Task for the AI\n",
        "\n",
        "Our goal is to construct and train an artificial neural network on thousands of images of handwritten digits so that it may successfully identify others when presented. The data that will be incorporated is the MNIST database which contains 60,000 images for training and 10,000 test images. We will use the Keras Python API with TensorFlow as the backend."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdJ727uGEkTq"
      },
      "source": [
        "<img src=\"https://github.com/jcanoj/UNED/blob/master/keras_mnist_tutorial/mnist.png?raw=1\" >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrK52-twEkTq"
      },
      "source": [
        "## Prerequisite Python Modules\n",
        "\n",
        "First, some software needs to be loaded into the Python environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "NvB1WAwaEkTs"
      },
      "outputs": [],
      "source": [
        "import numpy as np                   # advanced math library\n",
        "import matplotlib.pyplot as plt      # MATLAB like plotting routines\n",
        "import random                        # for generating random numbers\n",
        "\n",
        "from keras.datasets import mnist     # MNIST dataset is included in Keras\n",
        "from keras.models import Sequential  # Model type to be used\n",
        "\n",
        "from keras.layers.core import Dense, Dropout, Activation # Types of layers to be used in our model\n",
        "from keras.utils import np_utils                         # NumPy related tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTKvoU-AEkTt"
      },
      "source": [
        "## Loading Training Data\n",
        "\n",
        "The MNIST dataset is conveniently bundled within Keras, and we can easily analyze some of its features in Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "KeIRI1mSEkTu",
        "outputId": "65b94156-4989-4f4b-d705-41615fba89da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape (60000, 28, 28)\n",
            "y_train shape (60000,)\n",
            "X_test shape (10000, 28, 28)\n",
            "y_test shape (10000,)\n"
          ]
        }
      ],
      "source": [
        "# The MNIST data is split between 60,000 28 x 28 pixel training images and 10,000 28 x 28 pixel images\n",
        "# jcano - ojo a las minúsculas y mayúsculas:\n",
        "# X_train (mayúsculas) : son las imágenes\n",
        "# x_train (minúsculas) : es la clase (la etiqueta que identifica el número)\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(\"X_train shape\", X_train.shape)\n",
        "print(\"y_train shape\", y_train.shape)\n",
        "print(\"X_test shape\", X_test.shape)\n",
        "print(\"y_test shape\", y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFO458pNEkTv"
      },
      "source": [
        "Using matplotlib, we can plot some sample images from the training set directly into this Jupyter Notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "o6UZbrDnEkTw",
        "outputId": "452ac75b-1afc-4198-aa11-adbb50684a62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x648 with 9 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnIAAAKACAYAAAAYdJWHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeZQV1bn//88j4FWcGcS+ihATRNGoUYK65Ea9TjgFXBojDjHGiDGaOKBegwMYxa+JxuFnjIpKwIgaB3CIwasxqBijVzA4YgzhgoIMIogYJ5Dn98cpbjqsXd1dp+sMu877tZaruz/ndNUuPA/noU7t2ubuAgAAQHzWqfUAAAAAUB4aOQAAgEjRyAEAAESKRg4AACBSNHIAAACRopEDAACIFI1cHTGzUWZ2Z63HAcSOWgLyQz3VNxq5KjOzY81smpl9ZGYLzGyymQ2s0VjmmNknyVg+MrPHazEOoBx1Vku9zWyKmX1sZm+a2f61GAdQrjqrp8vM7FUzW2Vmo2oxhpjQyFWRmZ0j6TpJV0jqIWlrSb+SNLiGwzrc3TdM/juwhuMA2qwOa+luSX+R1FXShZLuN7PuNRoLkEkd1tMsSedLerRG+48KjVyVmNkmkn4q6XR3n+ju/3D3le7+iLufl/I795nZQjNbbmbPmNkOzR47xMzeMLMVZjbfzM5N8m5m9jsz+8DMlprZVDPj/zMKo95qycy2lbSrpJHu/om7PyDpVUlHVuL4gTzVWz1JkruPd/fJklZU4JALhzf46tlT0nqSJmX4ncmS+kjaXNJLkiY0e+x2Sae6+0aSdpT0xyQfLmmepO4q/ctqhKSW1mGbYGbvmdnjZrZzhrEBtVJvtbSDpNnu3vxN5+UkB+pdvdUTMqKRq56ukpa4+6q2/oK7j3X3Fe7+maRRknZO/vUkSSsl9TOzjd19mbu/1CxvktQr+VfVVE9fUPc4Sb0l9ZI0RdJ/m9mmmY8MqK56q6UNJS1fK1suaaMMxwTUSr3VEzKikaue9yV1M7OObXmymXUwsyvN7O9m9qGkOclD3ZKvR0o6RNJcM3vazPZM8qtUur7gcTObbWYXpO3D3f+UfBT0sbv/P0kfSPqP7IcGVFW91dJHkjZeK9tYfCyEONRbPSEjGrnq+bOkzyQNaePzj1XpQtP9JW2i0pkzSTJJcvcX3X2wSqe2H5R0b5KvcPfh7r6NpG9KOsfM9mvjPn3N9oE6Vm+19Lqkbcys+Rm4nZMcqHf1Vk/IiEauStx9uaRLJN1oZkPMrLOZdTKzg83s54Ff2Uil4npfUmeVZhNJksxsXTM7zsw2cfeVkj6UtDp57DAz+4qZmUof73yx5rHmzGxrM9sr2dZ6ZnaeSv+i+lO+Rw7kq95qyd3fkjRD0siklo6QtJOkB/I8bqAS6q2ekud2MrP1VOpROiZ11SG/oy4WGrkqcvdfSDpH0kWS3pP0jqQzVPpXy9rukDRX0nxJb0h6fq3HT5A0Jzm1/QOVrneTSheg/kGlj3v+LOlX7j4lsP2NJN0kaVmyj0GSDnb398s9PqBa6qyWJOkYSf1VqqcrJR3l7u+VdXBAldVhPd0q6RNJQ1W6nc8nyXYRYFxrCAAAECfOyAEAAESKRg4AACBSNHIAAACRopEDAACIVJtuAJjGzAZJul5SB0m3ufuVrTyfmRWod0vcvSaLnVNPKBp3r9l9KbPUE7WECKS+N5V9Ri65p8uNkg6W1E/SUDPrV+72gDoxtxY7pZ6A/FBPKKDU96b2fLQ6QNIsd5/t7p9Lukeluz0DyI56AvJDPaFhtKeR21KlmwauMS/J/oWZDTOzaWY2rR37AoqOegLy02o9UUsoinZdI9cW7j5G0hiJ6xCA9qKegHxQSyiK9pyRmy+pZ7Oft0oyANlRT0B+qCc0jPY0ci9K6mNmXzKzdVVaa/DhfIYFNBzqCcgP9YSGUfZHq+6+yszOkPTfKk3vHuvur+c2MqCBUE9AfqgnNBJzr96lAVyHgAhMd/f+tR5EW1BPqHe1vI9cFtQSIpD63sTKDgAAAJGikQMAAIgUjRwAAECkaOQAAAAiRSMHAAAQKRo5AACASNHIAQAARIpGDgAAIFI0cgAAAJGikQMAAIgUjRwAAECkaOQAAAAiRSMHAAAQKRo5AACASNHIAQAARKpjrQcAAACqp1OnTsF85syZwfxLX/pSMO/WrVswX7ZsWXkDQ1k4IwcAABApGjkAAIBI0cgBAABEikYOAAAgUjRyAAAAkWrXrFUzmyNphaQvJK1y9/55DCo23bt3D+b9+4f/OP793/89mI8ZMybTftdZJ70PX716daZtPfzww5nGNHny5EzbR+uop3ztsMMOwXzYsGHB/Nhjj828j7vuuiuYz5gxI/O2Qu6///5gvmLFily2X2TUU7pBgwYF8549ewZzdw/mjz32WDDffffdyxsYypLH7Uf2dfclOWwHAPUE5Il6QuHx0SoAAECk2tvIuaTHzWy6mYU/rwDQVtQTkB/qCQ2hvR+tDnT3+Wa2uaQnzOxNd3+m+ROSAqKIgNZRT0B+WqwnaglF0a4zcu4+P/m6WNIkSQMCzxnj7v250BRoGfUE5Ke1eqKWUBSWNhul1V8020DSOu6+Ivn+CUk/dffwNJbS75S3szpx2mmnBfODDjoomB966KGVHE6us1az+vGPfxzMb7rpporutwqm1+Iv9kasp6zS1odMm4F38803B/OmpqbcxpQmrf5mzZoVzLfddttgPnv27GD++eefB/MbbrghmNeqLt3darHfrPXUaLWU5vnnnw/maXdgSLP55punPrZ06dJM28L/SX1vas9Hqz0kTTKzNdu5q6U3HQAtop6A/FBPaBhlN3LuPlvSzjmOBWhY1BOQH+oJjYTbjwAAAESKRg4AACBSNHIAAACRymOJrmj17t07mB955JHB/JJLLgnmG264YTDPOnP0gw8+COZLloRXmEku5A1Km43cq1evYJ42IzDNIYccEswLMGsVNbbeeusF89GjRwfzs88+O5f9Ll++PPWxP/7xj8H82muvzbSPt99+O5hvvfXWwXznncOXef3whz8M5ldffXUwnzNnTjBnzWQ0d8sttwTzrLNWL7744tTH8qpX/BNn5AAAACJFIwcAABApGjkAAIBI0cgBAABEikYOAAAgUmWvtVrWzupsPbu//vWvwXybbbbJtJ20NU+nTJkSzF955ZVgPnXq1GA+adKkTONpyUUXXRTMu3Tpkmk7aX92abOeIlKTtVbLUW/1lNUee+wRzEeNGhXMDzzwwEzbT5uF+oc//CGYX3/99anbevbZZzPtu9K6desWzCdMmBDM0+o7bSbwgw8+WN7A1lKrtVazir2W8rLVVlsF8xkzZgTzTTfdNJh/8sknqfvo27dvMH/33XdbGV3DS31v4owcAABApGjkAAAAIkUjBwAAECkaOQAAgEjRyAEAAESKWasBWWetHn744cH8jTfeCOZp6y2iLjBrNWcDBgwI5mkzI7fYYotM2//ss8+C+QEHHBDM620Gap4222yzYP673/0umKfNuN9zzz1zGQ+zVovhZz/7WTAfPnx4MG9pHfAzzzwzmP/yl7/MPrDGwqxVAACAoqGRAwAAiBSNHAAAQKRo5AAAACJFIwcAABCpjq09wczGSjpM0mJ33zHJukj6raTekuZIOtrdl1VumJWRNrMmbSbXoYceGswfe+yx3MaEYityPe22227B/NFHHw3mXbt2DebLloUP/dprrw3mTz31VDAv8uzUNJ06dQrmG2ywQTBfsGBBJYdTcUWup3qSVmNpM1DTXodS+lqrKF9bzsiNkzRorewCSU+6ex9JTyY/A2jdOFFPQF7GiXpCg2u1kXP3ZyQtXSseLGl88v14SUNyHhdQSNQTkB/qCSj/Grke7r7mnPxCST1yGg/QiKgnID/UExpKq9fItcbdvaW7YpvZMEnD2rsfoBFQT0B+WqonaglFUe4ZuUVm1iRJydfFaU909zHu3j+WZY+AGqCegPy0qZ6oJRRFuWfkHpZ0oqQrk68P5TaiCjjqqKOCeZcuXYL56tWrg3k116VFQ4mqntL88Ic/DOZps1MXLlwYzG+66aZgfvnll5c3sAbSrVu3YL7TTjsF8wkTJlRyOLVSiHqqJ5MnTw7maesctzRrdciQ8CWLP/rRj7IPDJLacEbOzO6W9GdJfc1snpmdrFKBHGBmf5O0f/IzgFZQT0B+qCegDWfk3H1oykP75TwWoPCoJyA/1BPAyg4AAADRopEDAACIFI0cAABApNp9H7kYDBw4MJhvsskmuWw/bR3DpqamYD537txgvnLlylzGA1RSv379gvm3vvWtTNtJq4PLLrss85hQsmjRomA+ffr0YH7hhRcG86uuuiq3MQGoLM7IAQAARIpGDgAAIFI0cgAAAJGikQMAAIgUjRwAAECkGmLW6tSpU4P58ccfH8zTZrMedthhwXzw4MHB/JRTTgnmo0ePDubvv/9+ME9jZqmPVXpd2LR9P/3008H85ZdfruRwUEVpdbDhhhtm2s51112Xx3Aa0mabbRbMb7311mC+2267VXI4QJttscUWwTzt/fjOO++s5HAKgTNyAAAAkaKRAwAAiBSNHAAAQKRo5AAAACJFIwcAABCphpi1+sADDwTzK664IpinzVr9wQ9+EMzXWSfcD69evTqYp61vmFXaflvad17S9v36668H84MOOiiYL1iwILcxoT5NnDgxmD/++ONVHklxDBo0KJgPGTIk03bmz5+fx3DQoNLuXtDSe9NHH30UzF955ZVcxtSIOCMHAAAQKRo5AACASNHIAQAARIpGDgAAIFI0cgAAAJGikQMAAIhUq7cfMbOxkg6TtNjdd0yyUZJOkfRe8rQR7v77Sg2yUqZOnRrM33rrrWD+9a9/PZj36NEj036nTZsWzPv375/p+WlTv6X0RbL/8Y9/BPO0xe7TpC2cvsMOOwTzzp07Z9p+URW5ntJcdNFFwXzZsmVVHkl8DjzwwGB+ww03ZNpO2u2IRo8enXlM9aQR66meuHswb+n2Vx9++GEw5/Yj5WvLGblxkkI3LbrW3XdJ/qNIgLYZJ+oJyMs4UU9ocK02cu7+jKSlVRgLUHjUE5Af6glo3zVyZ5jZK2Y21sw2S3uSmQ0zs2lmFv58EIBEPQF5arWeqCUURbmN3E2SvixpF0kLJP0i7YnuPsbd+7t7+AIwANQTkJ821RO1hKIoq5Fz90Xu/oW7r5Z0q6QB+Q4LaBzUE5Af6gmNptVZqyFm1uTua1Y7P0LSa/kNqXq+//3vZ3r+4YcfHsx79eoVzNNm9DzyyCOZtp/2/JZmrabNKl2+fHkwv/POO1O3FfLFF18E87TZSqeeemowP//88zPtt4iKUk9oXceO4b9yBw4cGMzvvffeYL7xxhtn2u+vf/3rYH7zzTdn2k4MqKfqefTRR4P5t7/97dTf2XTTTYN5Wg08++yz2QfWYNpy+5G7Je0jqZuZzZM0UtI+ZraLJJc0R1L4XRrAv6CegPxQT0AbGjl3HxqIb6/AWIDCo56A/FBPACs7AAAARItGDgAAIFI0cgAAAJEqa9Zqo0qbPZqXG2+8sS63lYe0GbnMWkUj+clPfhLML7300ly2v2TJkmBeb38foBheffXVYN7SrNX1118/mO+1117BnFmrreOMHAAAQKRo5AAAACJFIwcAABApGjkAAIBI0cgBAABEilmrACrqmGOOCea33HJLMF+wYEEwr6W0Wdc77LBDMB8xYkQwX3fddXMZT9qf0eDBg4P5jBkzctkv0Nw777yT27ZOOumkYP6zn/0st30UFWfkAAAAIkUjBwAAECkaOQAAgEjRyAEAAESKRg4AACBS5u7V25lZ9XaGivriiy+C+erVq4P5rFmzgvn222+f25hyMt3d+9d6EG1Rq3paunRpMN90000zbWfkyJHBfObMmZnHFJK2duM3v/nNzNtqamoK5mnrRub19+pVV10VzK+//vpg/u677+ay37y4u9V6DG3Be1O+0t4fpPTaSHuP2G677XIZUwGkvjdxRg4AACBSNHIAAACRopEDAACIFI0cAABApGjkAAAAItXqWqtm1lPSHZJ6SHJJY9z9ejPrIum3knpLmiPpaHdfVrmhop6ss062fwNMmzatQiOJSxHq6eCDDw7mv/rVr4L51772tWB+6aWX5jamWsk6O/Xtt98O5scff3wwT6ubTz/9NNN+i6oI9QS0V1vejVdJGu7u/STtIel0M+sn6QJJT7p7H0lPJj8DaBn1BOSHekLDa7WRc/cF7v5S8v0KSTMlbSlpsKTxydPGSxpSqUECRUE9AfmhnoA2fLTanJn1lvQ1SS9I6uHuC5KHFqp0ajv0O8MkDSt/iEAxUU9AfrLWE7WEomjzhU5mtqGkBySd5e4fNn/MSxeKBC8Wcfcx7t4/lrvlA9VAPQH5KaeeqCUURZsaOTPrpFKRTHD3iUm8yMyaksebJC2uzBCBYqGegPxQT2h0bZm1apJulzTT3a9p9tDDkk6UdGXy9aGKjDAC/fr1C+bDhoXP2l9++eXBfMmSJbmNKU337t2D+YUXXphpO2lrqqblo0aNyrT9oipCPb3wwgvBfMiQ8GVId9xxRzAfMGBAME9bv7SW0mab/uMf/wjmDz0U/t83fvz4YP7Xv/61vIE1uCLUE9BebblGbi9JJ0h61cxmJNkIlQrkXjM7WdJcSUdXZohAoVBPQH6oJzS8Vhs5d39WkqU8vF++wwGKjXoC8kM9AazsAAAAEC0aOQAAgEjRyAEAAEQq0w2BEdarV69gfvrppwfzww8/PJivWrUq035LE7bC0taA7Ngx/L986623zrRvYG3vvPNOMN93332Dedqareeee24w/+pXv5ppPFOnTg3mjzzySKbtSNLjjz8ezN99993M2wKAPHFGDgAAIFI0cgAAAJGikQMAAIgUjRwAAECkaOQAAAAixazVGshrhug666T34WlrnuZlzpw5wTxtjcnly5dXcDSI0eTJkzPlAIrrueeeq/UQosUZOQAAgEjRyAEAAESKRg4AACBSNHIAAACRopEDAACIFLNWczBt2rRgnramY9paqzE5+uijg/n06dOrPBIAQD3p0KFDrYfQUDgjBwAAECkaOQAAgEjRyAEAAESKRg4AACBSNHIAAACRMndv+QlmPSXdIamHJJc0xt2vN7NRkk6R9F7y1BHu/vtWttXyzoDam+7u/Su1ceoJjcTdrVLbppbQYFLfm9py+5FVkoa7+0tmtpGk6Wb2RPLYte5+dV6jBBoA9QTkg1oC1IZGzt0XSFqQfL/CzGZK2rLSAwOKiHoC8kEtASWZrpEzs96SvibphSQ6w8xeMbOxZrZZyu8MM7NpZha+ay7QoKgnIB/UEhpZq9fI/d8TzTaU9LSk0e4+0cx6SFqi0rUJl0lqcvfvtbINrkNAvavoNXJrUE9oBJW8Rm4NagkNIvW9qU1n5Mysk6QHJE1w94mS5O6L3P0Ld18t6VZJA/IaLVBk1BOQD2oJaEMjZ2Ym6XZJM939mmZ5U7OnHSHptfyHBxQL9QTkg1oCStoya3UvSSdIetXMZiTZCElDzWwXlU5fz5F0akVGCBQL9QTkg1oClOEauVx2xnUIqH9VuUYuD9QT6l01rpHLA7WECLTvGjkAAADUHxo5AACASNHIAQAARIpGDgAAIFI0cgAAAJGikQMAAIgUjRwAAECkaOQAAAAiRSMHAAAQqbYs0ZWnJZLmJt93S35uFBxvHHrVegAZUE+NI8bjpZbi0WjHHOPxptZTVZfo+pcdm02LZSmkPHC8qKRG+/PmeFEpjfhn3WjHXLTj5aNVAACASNHIAQAARKqWjdyYGu67FjheVFKj/XlzvKiURvyzbrRjLtTx1uwaOQAAALQPH60CAABEikYOAAAgUlVv5MxskJn91cxmmdkF1d5/NZjZWDNbbGavNcu6mNkTZva35OtmtRxjnsysp5lNMbM3zOx1MzszyQt7zPWCeirea4t6qh3qqVivrUappao2cmbWQdKNkg6W1E/SUDPrV80xVMk4SYPWyi6Q9KS795H0ZPJzUaySNNzd+0naQ9Lpyf/XIh9zzVFPhX1tUU81QD0V8rXVELVU7TNyAyTNcvfZ7v65pHskDa7yGCrO3Z+RtHSteLCk8cn34yUNqeqgKsjdF7j7S8n3KyTNlLSlCnzMdYJ6KinUa4t6qhnqqaQwr61GqaVqN3JbSnqn2c/zkqwR9HD3Bcn3CyX1qOVgKsXMekv6mqQX1CDHXEPUU0lhX1vUU1VRTyWFfG0VuZaY7FADXrrnS+Hu+2JmG0p6QNJZ7v5h88eKesyovaK+tqgn1EIRX1tFr6VqN3LzJfVs9vNWSdYIFplZkyQlXxfXeDy5MrNOKhXKBHefmMSFPuY6QD2pmK8t6qkmqCcV77XVCLVU7UbuRUl9zOxLZraupGMkPVzlMdTKw5JOTL4/UdJDNRxLrszMJN0uaaa7X9PsocIec52gnkoK9dqinmqGeiopzGurUWqp6is7mNkhkq6T1EHSWHcfXdUBVIGZ3S1pH0ndJC2SNFLSg5LulbS1pLmSjnb3tS84jZKZDZQ0VdKrklYn8QiVrkUo5DHXC+qpeK8t6ql2qKdivbYapZZYogsAACBSTHYAAACIFI0cAABApGjkAAAAIkUjBwAAECkaOQAAgEjRyAEAAESKRg4AACBSNHIAAACRopEDAACIFI0cAABApGjkAAAAIkUjBwAAECkauTphZqPM7M5ajwMoAuoJyA/1VN9o5KrIzI41s2lm9pGZLTCzyWY2sEZjuczMXjWzVWY2qhZjANqjzuppipm9Z2YfmtnLZja4FuMAylVP9dRsTHubmZvZ5bUcR72jkasSMztH0nWSrpDUQ9LWkn4lqVZ/4c+SdL6kR2u0f6BsdVhPZ0pqcveNJQ2TdKeZNdVoLEAmdVhPMrNOkq6X9EKtxhALGrkqMLNNJP1U0unuPtHd/+HuK939EXc/L+V37jOzhWa23MyeMbMdmj12iJm9YWYrzGy+mZ2b5N3M7Hdm9oGZLTWzqWYW/H/s7uPdfbKkFRU4ZKBi6rSeXnH3VWt+lNRJUs9cDxyogHqsp8RwSY9LejPHwy0kGrnq2FPSepImZfidyZL6SNpc0kuSJjR77HZJp7r7RpJ2lPTHJB8uaZ6k7ir9q2qESm8qQJHUZT0lb1KfqnQG4SlJ0zKMD6iVuqsnM+sl6XsqNZhoRcdaD6BBdJW0pNm/2Fvl7mPXfJ9cw7bMzDZx9+WSVkrqZ2Yvu/syScuSp66U1CSpl7vPkjQ1rwMA6khd1pO7H5Z8HLS/pO3dfXWWgwJqpB7r6f+TdLG7f2Rm2Y6mAXFGrjrel9TNzNrUOJtZBzO70sz+bmYfSpqTPNQt+XqkpEMkzTWzp81szyS/SqVr3x43s9lmdkF+hwDUjbqtp+QjqcmSDjSzb2Y4JqBW6qqezOxwSRu5+2/LPJ6GQyNXHX+W9JmkIW18/rEqXWS6v6RNJPVOcpMkd3/R3QerdFr7QUn3JvkKdx/u7ttI+qakc8xsv7wOAqgTMdRTR0lfbuNzgVqqt3raT1L/5Bq8hZK+LeksM3uonINrBDRyVZCcbr5E0o1mNsTMOptZJzM72Mx+HviVjVQqrPcldVZpJpEkyczWNbPjktPYKyV9KGl18thhZvYVK52LXi7pizWPrS3Z/3oqvQY6mtl6ZtYhv6MGKqPe6snMtkv2vX4yjuMlfUPS0/keOZC/eqsnSRdL2lbSLsl/D0u6VdJJOR1y4dDIVYm7/0LSOZIukvSepHcknaHSv1jWdoekuZLmS3pD0vNrPX6CpDnJae0fSDouyftI+oOkj1T6V9av3H1KypBulfSJpKGSLky+P6GcYwOqrc7qySSNkrQ4GcuZkr7t7i+VeXhAVdVTPSVn7hau+U+l96Z/uPvS9h1lcZk7kxoBAABixBk5AACASNHIAQAARIpGDgAAIFI0cgAAAJFq18oOZjZIpUVtO0i6zd2vbOX5zKxAvVvi7t1rsWPqCUXj7jW7LX+WeqKWEIHU96ayz8gl9xy7UdLBkvpJGmpm/crdHlAn5tZip9QTkB/qCQWU+t7Uno9WB0ia5e6z3f1zSfeodLdnANlRT0B+qCc0jPY0cluqdNPANeYl2b8ws2FmNs3MprVjX0DRUU9AflqtJ2oJRdGua+Tawt3HSBojcR0C0F7UE5APaglF0Z4zcvMl9Wz281ZJBiA76gnID/WEhtGeRu5FSX3M7Etmtq6kY1Ra3BZAdtQTkB/qCQ2j7I9W3X2VmZ0h6b9Vmt491t1fz21kQAOhnoD8UE9oJOZevUsDuA4BEZju7v1rPYi2oJ5Q72p5H7ksqCVEIPW9iZUdAAAAIkUjBwAAECkaOQAAgEjRyAEAAESKRg4AACBSNHIAAACRopEDAACIFI0cAABApGjkAAAAIkUjBwAAECkaOQAAgEjRyAEAAESKRg4AACBSNHIAAACR6ljrAeCfLrnkkmB+8cUXB/MtttgidVvvv/9+LmMC6t0+++wTzEeOHJnp+Xm69NJLMz1/1KhRlRkIgMLjjBwAAECkaOQAAAAiRSMHAAAQKRo5AACASNHIAQAARIpZq3Vk++23D+YdOnQI5uPGjUvd1uGHH57HkIC6kTazM212ai1lHdPee+8dzPfdd988hgNURNeuXYP5BhtskPo7b7/9dqWGI0nq1KlTML/iiiuC+bnnnhvMzznnnGB+7bXXljewCmpXI2dmcyStkPSFpFXu3j+PQQGNiHoC8kM9oVHkcUZuX3dfksN2AFBPQJ6oJxQe18gBAABEqr2NnEt63Mymm9mw0BPMbJiZTTOzae3cF1B01BOQnxbriVpCUbT3o9WB7j7fzDaX9ISZvenuzzR/gruPkTRGkszM27k/oMioJyA/LdYTtYSiaFcj5+7zk6+LzWySpAGSnmn5t5CVmQXzXr16pf7OeuutF8w//fTTXMaE/FFP+XrqqaeC+dNPP13dgTSTdf3XKVOmBHNms7aOeqq8n//858G8pdfnnnvuGcwXLVqUy5h69OgRzNNmoa5evTqYz5kzJ5fxVEPZH62a2QZmttGa7yUdKOm1vAYGNBLqCabfUV4AACAASURBVMgP9YRG0p4zcj0kTUrOFnWUdJe7P5bLqIDGQz0B+aGe0DDKbuTcfbaknXMcC9CwqCcgP9QTGgm3HwEAAIgUjRwAAECkzL16s66Z4t2y4447Lpj/5je/ybytAQMGBPNp07hlUiumx7KUD/UUn6yzU9OkzWSvN+4exUCppZb169cvmD///PPB/PXXX0/dVtqs1bxsvfXWwfx///d/g/ldd90VzL/zne8E82r2TGtJfW/ijBwAAECkaOQAAAAiRSMHAAAQKRo5AACASNHIAQAARKpda60iX/Pmzav1EABUUNr6r0A9u+WWW4L5BhtsEMyzzsLO02mnnZbp+Y888kgwr+Hs1Mw4IwcAABApGjkAAIBI0cgBAABEikYOAAAgUjRyAAAAkWLWagTS1lVsaVZNz549gzlrrQK1k7bWKlAPNt1002DetWvXYP7yyy8H8+uvvz63MaXp3bt3MD/qqKMybaeldWFjwRk5AACASNHIAQAARIpGDgAAIFI0cgAAAJGikQMAAIgUs1YjUM6ab0OGDAnmDz74YG77AJBN1jUoWZsVlZB2J4QrrrgimPft2zeYjxo1KpgvWrSorHFlcdBBBwXzbbbZJpg/99xzwfytt97KbUy10uoZOTMba2aLzey1ZlkXM3vCzP6WfN2sssMEioF6AvJDPQFt+2h1nKRBa2UXSHrS3ftIejL5GUDrxol6AvIyTtQTGlyrjZy7PyNp6VrxYEnjk+/HSwp/jgfgX1BPQH6oJ6D8a+R6uPuC5PuFknqkPdHMhkkaVuZ+gEZAPQH5aVM9UUsoinZPdnB3N7PUK+XdfYykMZLU0vMAUE9AnlqqJ2oJRVFuI7fIzJrcfYGZNUlanOeg0H4HHHBAMO/QoUMwX7VqVSWHg5ZRT5FKWzt15MiRmbaTNjv10ksvzTgiiHpqVefOnYP5qaeeGsw/+OCDYP7LX/4ytzGl2XDDDYP5WWedFcxXrlwZzO+5555Mz49JufeRe1jSicn3J0p6KJ/hAA2JegLyQz2hobTl9iN3S/qzpL5mNs/MTpZ0paQDzOxvkvZPfgbQCuoJyA/1BLTho1V3H5ry0H45jwUoPOoJyA/1BLBEFwAAQLRo5AAAACLFWqt1ZP78+cF82bJlwXyzzdJXnll//fVzGRNQL9JmiGZ9ftoM0Za2n3UWalZps1NZaxXtsdVWWwXztBmfadJms6a9N+XpW9/6VjDfdtttg3na++iNN96Y25jqDWfkAAAAIkUjBwAAECkaOQAAgEjRyAEAAESKRg4AACBSzFqtI7NmzQrmixYtCuYtzVrdZJNNgrmZZR8YUAfyWte00jNQW0L9oRLS1tD+8Y9/HMzPPvvsYD59+vRg/tBDtVvl7KCDDsr0/JtuuqlCI6lfnJEDAACIFI0cAABApGjkAAAAIkUjBwAAECkaOQAAgEgxa7XBHHnkkcH8nnvuqfJIgMaTdf1XoC3SZqcOHz48mH/22WfBfOjQocF85cqV5Q0sg4033jiY77777sF8yZIlwfzaa6/NbUyx4IwcAABApGjkAAAAIkUjBwAAECkaOQAAgEjRyAEAAESKRg4AACBSrd5+xMzGSjpM0mJ33zHJRkk6RdJ7ydNGuPvvKzXIRjdmzJhgfs0112TeVvfu3ds7HLQD9VS+UaNGZcrTpN0CJC1vyciRIzM9f8qUKcF83333DebclqRljVZPO+64YzD/r//6r0zbSauZv//971mHlJvvfOc7wXzrrbcO5r/+9a+D+aeffprbmGLRljNy4yQNCuTXuvsuyX+FKBKgCsaJegLyMk7UExpcq42cuz8jaWkVxgIUHvUE5Id6Atp3jdwZZvaKmY01s83SnmRmw8xsmplNa8e+gKKjnoD8tFpP1BKKotxG7iZJX5a0i6QFkn6R9kR3H+Pu/d29f5n7AoqOegLy06Z6opZQFGU1cu6+yN2/cPfVkm6VNCDfYQGNg3oC8kM9odG0Oms1xMya3H1B8uMRkl7Lb0hY2+rVq2s9BFQQ9VRdaTNBy5khmvY7abNT06TNfmXWanZFrqf77rsvmKfdjeCiiy4K5ldffXVuY8rL4Ycfnun5L7zwQoVGEp+23H7kbkn7SOpmZvMkjZS0j5ntIsklzZF0agXHCBQG9QTkh3oC2tDIufvQQHx7BcYCFB71BOSHegJY2QEAACBaNHIAAACRopEDAACIVFmzVlFdzM4B6lParFIzC+buHszT1nnNa31ZFMO//du/ZXr+NttsE8w7deoUzD/77LPMYwrZaKONUh/beeedg/mee+4ZzD/88MNgnnVmeJFxRg4AACBSNHIAAACRopEDAACIFI0cAABApGjkAAAAIsWs1Qj85S9/qfUQANTA3nvvXeshoI7cf//9wXz48OHB/Hvf+14w32OPPYL5ypUryxvYWjp37pz6WJ8+fTJt609/+lMwnzVrVqbtFBln5AAAACJFIwcAABApGjkAAIBI0cgBAABEikYOAAAgUpa29l9FdmZWvZ0VyLrrrhvMP/3008zbmjFjRjBPm8X0+eefZ95H5Ka7e/9aD6ItalVPaeuCpq19eOmllwbzIq8XmvZnkfZnl2bfffcN5mlrvNYbdw8vOltnYnlvSlvDd6uttgrmxx57bDDfddddg3naeqdZpb1nSVL37t0zbev4448P5nfffXem7RRA6nsTZ+QAAAAiRSMHAAAQKRo5AACASNHIAQAARIpGDgAAIFKtzlo1s56S7pDUQ5JLGuPu15tZF0m/ldRb0hxJR7v7sla2FcXMoHqT56zVNGmz6Z555pnc9hGJis5aLUI95TXTPfYZmVJ+s1OLOrO30rNW86on3pvytfvuu6c+9txzzwXzDz/8MJh/9atfDebz5s3LPrC4tWvW6ipJw929n6Q9JJ1uZv0kXSDpSXfvI+nJ5GcALaOegPxQT2h4rTZy7r7A3V9Kvl8haaakLSUNljQ+edp4SUMqNUigKKgnID/UEyB1zPJkM+st6WuSXpDUw90XJA8tVOnUduh3hkkaVv4QgWKinoD8ZK0naglF0ebJDma2oaQHJJ3l7v/yYbaXLpoJXmPg7mPcvX8sd8sHqoF6AvJTTj1RSyiKNjVyZtZJpSKZ4O4Tk3iRmTUljzdJWlyZIQLFQj0B+aGe0Oha/WjVSou73S5pprtf0+yhhyWdKOnK5OtDFRkhUq1atSr1sY4dM31qjiopQj2lzbAcOXJkpu2kzfjMut+s9t5772CedaZpOYo6O7VWilBPRbTjjjtm/p3PPvssmDfg7NTM2vJuv5ekEyS9amZrVlwfoVKB3GtmJ0uaK+noygwRKBTqCcgP9YSG12oj5+7PSkq7F9B++Q4HKDbqCcgP9QSwsgMAAEC0aOQAAAAiRSMHAAAQKaY2RuDzzz8P5ldddVXq7/zkJz+p1HDQ4LLOsMw6m7XS28lT2rqwTz/9dDBndioaQdrM8JYMHTq0AiNpDJyRAwAAiBSNHAAAQKRo5AAAACJFIwcAABApGjkAAIBImbtXb2dm1dtZA+jVq1fqYy+++GIw79ChQzDffffdg/msWbOyDyxu0929f60H0Rax11OtZr+maWkt17TZqWk5Stw9bdWFuhJ7LdWbO+64I/Wx4447Lph37do1mH/wwQe5jKkAUt+bOCMHAAAQKRo5AACASNHIAQAARIpGDgAAIFI0cgAAAJFi1irwr5i1CuSEWauN6bzzzkt9bLfddgvmJ510UjD/5JNPchlTATBrFQAAoGho5AAAACJFIwcAABApGjkAAIBI0cgBAABEqtVZq2bWU9IdknpIcklj3P16Mxsl6RRJ7yVPHeHuv29lW8wMQr2r6KxV6gmNpJKzVqklNJjU96aObfjlVZKGu/tLZraRpOlm9kTy2LXufnVeowQaAPUE5INaAtSGRs7dF0hakHy/wsxmStqy0gMDioh6AvJBLQElma6RM7Pekr4m6YUkOsPMXjGzsWa2WcrvDDOzaWY2rV0jBQqGegLyQS2hkbV5ZQcz21DS05JGu/tEM+shaYlK1yZcJqnJ3b/Xyja4DgH1riorO1BPaATVWNmBWkKDaN/KDmbWSdIDkia4+0RJcvdF7v6Fu6+WdKukAXmNFigy6gnIB7UEtKGRMzOTdLukme5+TbO8qdnTjpD0Wv7DA4qFegLyQS0BJW2ZtbqXpBMkvWpmM5JshKShZraLSqev50g6tSIjBIqFegLyQS0BynCNXC474zoE1L+qXCOXB+oJ9a4a18jlgVpCBNp3jRwAAADqD40cAABApGjkAAAAIkUjBwAAECkaOQAAgEjRyAEAAESKRg4AACBSNHIAAACRopEDAACIVFuW6MrTEklzk++7JT83Co43Dr1qPYAMqKfGEePxUkvxaLRjjvF4U+upqkt0/cuOzabFshRSHjheVFKj/XlzvKiURvyzbrRjLtrx8tEqAABApGjkAAAAIlXLRm5MDfddCxwvKqnR/rw5XlRKI/5ZN9oxF+p4a3aNHAAAANqHj1YBAAAiRSMHAAAQqao3cmY2yMz+amazzOyCau+/GsxsrJktNrPXmmVdzOwJM/tb8nWzWo4xT2bW08ymmNkbZva6mZ2Z5IU95npBPRXvtUU91Q71VKzXVqPUUlUbOTPrIOlGSQdL6idpqJn1q+YYqmScpEFrZRdIetLd+0h6Mvm5KFZJGu7u/STtIen05P9rkY+55qinwr62qKcaoJ4K+dpqiFqq9hm5AZJmuftsd/9c0j2SBld5DBXn7s9IWrpWPFjS+OT78ZKGVHVQFeTuC9z9peT7FZJmStpSBT7mOkE9lRTqtUU91Qz1VFKY11aj1FK1G7ktJb3T7Od5SdYIerj7guT7hZJ61HIwlWJmvSV9TdILapBjriHqqaSwry3qqaqop5JCvraKXEtMdqgBL93zpXD3fTGzDSU9IOksd/+w+WNFPWbUXlFfW9QTaqGIr62i11K1G7n5kno2+3mrJGsEi8ysSZKSr4trPJ5cmVknlQplgrtPTOJCH3MdoJ5UzNcW9VQT1JOK99pqhFqqdiP3oqQ+ZvYlM1tX0jGSHq7yGGrlYUknJt+fKOmhGo4lV2Zmkm6XNNPdr2n2UGGPuU5QTyWFem1RTzVDPZUU5rXVKLVU9ZUdzOwQSddJ6iBprLuPruoAqsDM7pa0j6RukhZJGinpQUn3Stpa0lxJR7v72hecRsnMBkqaKulVSauTeIRK1yIU8pjrBfVUvNcW9VQ71FOxXluNUkss0QUAABApJjsAAABEikYOAAAgUjRyAAAAkaKRAwAAiBSNHAAAQKRo5AAAACJFIwcAABApGjkAAIBI0cgBAABEikYOAAAgUjRyAAAAkaKRAwAAiBSNXJ0ws1FmdmetxwEUAfUE5Id6qm80clVkZsea2TQz+8jMFpjZZDMbWKOxTDGz98zsQzN72cwG12IcQLnqrJ56JzX1sZm9aWb712IcQLnqrJ54f8qARq5KzOwcSddJukJSD0lbS/qVpFq9QM+U1OTuG0saJulOM2uq0ViATOqwnu6W9BdJXSVdKOl+M+teo7EAmdRhPfH+lAGNXBWY2SaSfirpdHef6O7/cPeV7v6Iu5+X8jv3mdlCM1tuZs+Y2Q7NHjvEzN4wsxVmNt/Mzk3ybmb2OzP7wMyWmtlUMwv+P3b3V9x91ZofJXWS1DPXAwcqoN7qycy2lbSrpJHu/om7PyDpVUlHVuL4gTzVWz1JvD9lRSNXHXtKWk/SpAy/M1lSH0mbS3pJ0oRmj90u6VR330jSjpL+mOTDJc2T1F2lf1WNUKkIgpKi+lTSC5KekjQtw/iAWqm3etpB0mx3X9EseznJgXpXb/UkifenLDrWegANoqukJc3+hdEqdx+75nszGyVpmZlt4u7LJa2U1M/MXnb3ZZKWJU9dKalJUi93nyVpaiv7OMzMOknaX9L27r46y0EBNVJv9bShpOVrZcslbdnW8QE1VG/1tGYfvD+1EWfkquN9Sd3MrE2Ns5l1MLMrzezvZvahpDnJQ92Sr0dKOkTSXDN72sz2TPKrJM2S9LiZzTazC1rbV3IKfbKkA83smxmOCaiVequnjyRtvFa2saQVgecC9abe6un/8P7UNjRy1fFnSZ9JGtLG5x+r0kWm+0vaRFLvJDdJcvcX3X2wSqe1H5R0b5KvcPfh7r6NpG9KOsfM9mvjPjtK+nIbnwvUUr3V0+uStjGzjZplOyc5UO/qrZ5CeH9qAY1cFSSnmy+RdKOZDTGzzmbWycwONrOfB35lI5UK631JnVWaSSRJMrN1zey45DT2SkkfSlqdPHaYmX3FzEylj3a+WPNYc2a2XbLv9ZNxHC/pG5KezvfIgfzVWz25+1uSZkgaaWbrmdkRknaS9ECexw1UQr3VE+9P2dHIVYm7/0LSOZIukvSepHcknaHSv1jWdoekuZLmS3pD0vNrPX6CpDnJae0fSDouyftI+oNKH/X8WdKv3H1KYPsmaZSkxclYzpT0bXd/qczDA6qqzupJko6R1F+l64GulHSUu79X1sEBVVZn9cT7U0bmnjppBAAAAHWMM3IAAACRopEDAACIFI0cAABApGjkAAAAItWulR3MbJCk6yV1kHSbu1/ZyvOZWYF6t8Tda7LYOfWEonF3q9W+s9QTtYQIpL43lX1Gzsw6SLpR0sGS+kkaamb9yt0eUCfm1mKn1BOQH+oJBZT63tSej1YHSJrl7rPd/XNJ96h0t2cA2VFPQH6oJzSM9jRyW6p008A15imwSLSZDTOzaWY2rR37AoqOegLy02o9UUsoinZdI9cW7j5G0hiJ6xCA9qKegHxQSyiK9pyRmy+pZ7Oft0oyANlRT0B+qCc0jPY0ci9K6mNmXzKzdVVaa/DhfIYFNBzqCcgP9YSGUfZHq+6+yszOkPTfKk3vHuvur+c2MqCBUE9AfqgnNBJzr96lAVyHgAhMd/f+tR5EW1BPqHe1vI9cFtQSIpD63sTKDgAAAJGikQMAAIgUjRwAAECkaOQAAAAiRSMHAAAQKRo5AACASNHIAQAARIpGDgAAIFI0cgAAAJGikQMAAIgUjRwAAECkaOQAAAAiRSMHAAAQKRo5AACASNHIAQAARKpjrQeA8m2wwQapj2233XbBvHv37sF8yJAhmfa9/fbbB/O9994703YAAED5OCMHAAAQKRo5AACASNHIAQAARIpGDgAAIFI0cgAAAJFq16xVM5sjaYWkLyStcvf+eQyqKL7xjW8E85/85CfBvFu3bpm237lz59TH+vbtG8zNLJi7ey7P/81vfhPMTzjhhGCOf6KeynPqqacG88suuyyYp9XZ22+/nbqPa665JpjfeOONwfyLL75I3RaqI+Z6uuGGG4L5uuuuG8wvv/zyYH7yyScH84svvjiY33bbbcH8o48+CubluPXWW4P5m2++mds+Gk0etx/Z192X5LAdANQTkCfqCYXHR6sAAACRam8j55IeN7PpZjYs9AQzG2Zm08xsWjv3BRQd9QTkp8V6opZQFO39aHWgu883s80lPWFmb7r7M82f4O5jJI2RJDMLX1gFQKKegDy1WE/UEoqiXWfk3H1+8nWxpEmSBuQxKKARUU9AfqgnNApLm33Y6i+abSBpHXdfkXz/hKSfuvtjLfxO1P/qGTRoUDAfP358ME9b1zSvGaJpz2/pd95///1g3rVr11zG9PHHHwfzr3/968G8DmcqTa/F7LZGrKesNt9882D+2muvBfOss8DLMWfOnGB+4YUXBvO77767gqOpP+6e/pdUBWWtp1rV0rbbbhvMn3322WDepUuXYJ717+msKr19Kf199Nprrw3maXVfYKnvTe35aLWHpEnJ/+COku5q6U0HQIuoJyA/1BMaRtmNnLvPlrRzjmMBGhb1BOSHekIj4fYjAAAAkaKRAwAAiBSNHAAAQKTyWKKrcI444ohgfv/99wfztJk7WfOpU6cG85kzZwbzJUvSV56ZNGlSME9b//Xqq69O3VZI2jF85zvfCeZ1ODsVkTn33HODeV6zUxcvXpz6WNqsvd69ewfztBl46623XjD/9a9/3fLggDKk3aVgypQpwfzQQw8N5i2t652X7373u8H8yCOPDOYnnXRSME977ysyzsgBAABEikYOAAAgUjRyAAAAkaKRAwAAiBSNHAAAQKTKXmu1rJ1FvjbkG2+8Ecz79u0bzC+55JJgPnr06FzGs8EGG6Q+dsEFFwTztBlAaceQNltv4sSJwfyoo45KHVMkarLWajlir6c0V155ZTA/++yzg3mnTp0ybT9ttvdFF12U+jvPPfdcMP/5z38ezNPWZf7888+D+V133RXMv/e976WOKQa1Wms1q3qrpb322iuYX3bZZcH8uuuuC+bPP/98ME+bod3U1BTMO3ToEMzTZrkecsghwbyl38m6nmvaXSSOOeaY1H1HLvW9iTNyAAAAkaKRAwAAiBSNHAAAQKRo5AAAACJFIwcAABApZq1m0KtXr2CeNkP0tNNOq+Rw9Prrr6c+lnUWatrr4MEHHwzmaWuqfvzxx6ljigSzVqtk4MCBwfzJJ58M5mmzU5999tlgfv755wfz2bNnB/OW1lpNk7YG5RlnnBHM02bkpu37hBNOCOZPPPFEG0ZXe8xaxdrS3lMGDx4czFevXh3M77vvvmDOrFUAAABEg0YOAAAgUjRyAAAAkaKRAwAAiBSNHAAAQKQ6tvYEMxsr6TBJi919xyTrIum3knpLmiPpaHdfVrlh1oe5c+cG86yzU7t37x7Mjz/++GCeNqN0++23T93HzJkzg3narNL33nsvmBdg7dS6Qj3904UXXhjM02anrly5MpjfcsstwTxtnck8pdXTvffeG8wPOOCAYL7ffvsF86uuuiqY77HHHsH8008/DeZFRT1VR+/evYP5FltsEcxbmjma9lpPm52a9v737rvvpu6j0bTljNw4SWuvAH2BpCfdvY+kJ5OfAbRunKgnIC/jRD2hwbXayLn7M5KWrhUPljQ++X68pCE5jwsoJOoJyA/1BLTho9UUPdx9QfL9Qkk90p5oZsMkDStzP0AjoJ6A/LSpnqglFEW5jdz/cXdv6a7Y7j5G0hiJu2cDraGegPy0VE/UEoqi3Fmri8ysSZKSr9nXtgGwBvUE5Id6QkMp94zcw5JOlHRl8vWh3EYUoSOOOCKY/8d//EemfNdddw3mabN2WlonN22t1bvuuiuYn3POOanbQsUVup6+8pWvBPM999wz03bGjBkTzCdMmJB5TJU2Z86cYH7eeecF8//5n/8J5jvttFMwHzFiRDC/5JJLWh9c8RW6nvLQs2fPYJ52B4YTTzwxmPfokXoVSG4mTpwYzEeOHFnxfcei1TNyZna3pD9L6mtm88zsZJUK5AAz+5uk/ZOfAbSCegLyQz0BbTgj5+5DUx4K3wwGQCrqCcgP9QSwsgMAAEC0aOQAAAAiRSMHAAAQqXbfR66RpK2FOn78+GBuZsE8bbZp2vPTZH2+lL4+65IlSzJvC2iLs88+O5hvvPHGmbYzbdq0PIZTUzNmzAjmTz75ZDA/6KCDgvlJJ50UzJm1iuZ22223YD558uRg3qVLl2Ce9b2sJbfeemumMT3xxBPB/JNPPsm876LijBwAAECkaOQAAAAiRSMHAAAQKRo5AACASNHIAQAARIpZqxlMnTo1mL/33nvBvHv37sE860yfcmYGvfHGG8G8W7duwfyaa64J5m+++Wam/ab9WUyaNCnTdlAcHTp0yPT81atXB/MXX3wxj+HUpbT6S5u12rlz52C+xRZbBPOFCxeWNzBEYdNNNw3maesQd+3aNdP2f//73wfzk08+OZgvXrw40/bRPpyRAwAAiBSNHAAAQKRo5AAAACJFIwcAABApGjkAAIBIMWs1g7lz5wbztLXgtttuu0zbT1vvdPTo0cG8X79+qds65ZRTgvk999wTzIcMGRLMzzrrrGCeNpP2+uuvD+YzZ84M5llnxSI+3/rWtzI9P23NxbSZ2EWwYMGCTM/fbLPNgvn+++8fzO+8887MY0I81l133WCetnZq1jshHHLIIcH8tddeC+ZTpkxJ3dZll12WaVtoHWfkAAAAIkUjBwAAECkaOQAAgEjRyAEAAESKRg4AACBSrTZyZjbWzBab2WvNslFmNt/MZiT/hae0APgX1BOQH+oJkKy1achm9g1JH0m6w913TLJRkj5y96sz7cws++rvOUi7DUiRb33RrVu3TM9P+zNKW7Q77XYlvXv3DuZTp04N5uecc04wr+H/m+nu3r9SGy9CPWW1dOnSYJ620PfFF18czNNuw1MEO+64YzB/5ZVXMm3npptuCuann3565jHlwd2tktvPq55iqaWs9t5772B+2mmnBfNDDz00mHfu3DmYZ72NiZR+q520W+TceOONwXzevHmZ9x251PemVs/IufszksJ/EwPIhHoC8kM9Ae27Ru4MM3slObUdvjslgLainoD8UE9oGOU2cjdJ+rKkXSQtkPSLtCea2TAzm2Zm08rcF1B01BOQnzbVE7WEoiirkXP3Re7+hbuvlnSrpAEtPHeMu/ev5HVHQMyoJyA/ba0naglFUVYjZ2ZNzX48QhKLpAFlop6A/FBPaDRtmbV6t6R9JHWTtEjSyOTnXSS5pDmSTnX3Vld9rvTMoJtvvjmYpy0g36FDh0oOp9DSZrlOnDgxmPft2zeYP/vss8E8bTbr9OnT2zC6dqn0rNVo6ikv77//fjBPW/idWav/lHXW6ne/+91gfscdd2QdUi6qMGs1l3qKpZYqrampKZj/53/+ZzC/6KKLgnmfPn0y79ss/FJJ+/tjxIgRwfy2227LvO9IpL43dWztN919aCC+vd1DAhoQ9QTkh3oCWNkBAAAgWjRyAAAAkaKRAwAAiBSNHAAAQKRanewQk0mTJgXzvfbaK5hfeOGFwbzIs+PykrYW6sEHHxzMX3jhhWA+cODAYP79738/mFdh1ipqbNdd2zzCkwAAC0tJREFUd631EKI1e/bsWg8BEUtbB3XChAmZ8qFDQ3NQSn72s58F86222iqYd+3aNZjfcsstwfzrX/96MD/33HOD+YoVK4J5TDgjBwAAECkaOQAAgEjRyAEAAESKRg4AACBSNHIAAACRKtSs1d/85jfBPG3Wy5lnnhnMH3vssWDOjMnWpc027d69ezBPW18PjWunnXYK5uuvv34w/+STTyo5nKo46aSTMj1/4cKFwfzFF1/MYzhAu9x9992pj6W9j55xxhnB/JhjjgnmXbp0CeYnn3xyMF++fHkwP//884N5TDgjBwAAECkaOQAAgEjRyAEAAESKRg4AACBSNHIAAACRKtSs1YkTJwbztJmUabNZH3300WB+ySWXZNrvkiVLgnlMtttuu2CeNkM4bZ1Md8+037R1cxGf3/3ud8H8hBNOCOZf/vKXg3naa+tPf/pTeQOrgdNPPz2Yp83YS/PUU08F888++yzrkICqeuutt4L5j3/842D+wAMPBPP77rsvmKfNZv3Rj34UzNPWJ7755puDeT3ijBwAAECkaOQAAAAiRSMHAAAQKRo5AACASLXayJlZTzObYmZvmNnrZnZmkncxsyfM7G/J180qP1wgbtQTkB/qCZCstdmEZtYkqcndXzKzjSRNlzRE0nclLXX3K83sAkmbuft/tbKtbFMXM/rGN74RzH/xi18E89122y2Yp/2ZpK0LmjZrdebMmcH8tttuC+Yff/xxME/T0vMvuOCCYH7kkUcG8759+wbztGNO+zNaZ53wvw2efvrpYP6DH/wgmL/55pvBvAqmu3v/Sm08pnrKS9pMzRtuuCHTdkaPHh3ML7744sxjqrTDDjssmKfNwOvUqVMwv//++4P5iSeeGMzrbd1Zd6/oYsp51VMstYR/2muvvYJ52l0nNtpoo2A+Z86cYD5w4MBgvmDBgtYHVxmp702tnpFz9wXu/lLy/QpJMyVtKWmwpPHJ08arVDwAWkA9AfmhnoCM18iZWW/p/2/vfkKrzM44jv8epnYh6WI0EoKa2sUoiIsKEgsdpdiF1k3qZuyAVSiYLlrpYDaDmxbL7MS2iy78E3GEUBnxz2TRzTCUGhdKbSjNtCKOpaIlo46zmOzKNE8X9w1k5JzJvcl73/ee834/G2+e/LnnZN7f5CF5n3u0XdIdSQPuvtCafiJpoNSVAZkjT0B5yBOaqu0XBDazPklXJb3l7p8v/pObu3vsV9NmNippdKULBXJCnoDyLCdPZAm5aOs3cma2Sq2QTLj7wg1hT4v7ExbuU3gW+lx3P+vuO7p53xGQEvIElGe5eSJLyEU7U6smaVzSPXc/vehdk5IW7rg9Iun98pcH5IU8AeUhT0B7U6uvS5qSNCNpviifUOs+hPckDUl6JOkNd/9sia9Vy2RQf39/sH7o0KFgPTbxuW7dumC90ynX58+fB+udTpxNT09H3zcyMtLRmjrdQ+zjb926FawfP348WP+qPdSk21OryeepU7Fpsdh0WWxaLHZG4759+4L12DTacgwPDwfrY2NjwfqBAweC9cePHwfrFy9eDNZPnToVrPfadGpMBVOrpeQplSx1atOmTcF67NzR2P+PJycng/W5ubllrasMq1evDtYfPHgQrA8ODgbrsZ9lsanY27dvt7G6roj+bFryHjl3vyUpFsbvr2RVQNOQJ6A85AngZAcAAIBk0cgBAAAkikYOAAAgUTRyAAAAiVpyarXUJ0t8Mmh0NPzakbEJtdhZrmvXrg3WO50QffHiRbC+nOeITdJOTEwE67GzUG/evNnRx/egrk6tlin1PO3YEf42x6ZZY1PjT548CdbHx8eD9WfPgi/Rp6GhoWBdik+yx7IZOyM1Nr0d20Pquj21WpbUsxSzbdu2YD326gJ9fX3B+szMTLAemyS/fPlydE2xc4hjzx2zYcOGYH3nzp3Beuxn3/3794P1Xbt2Betf9XO3y5Z/1ioAAAB6E40cAABAomjkAAAAEkUjBwAAkCgaOQAAgEQxtdpFsbNcY2e47d69O1jfsmVLsH7+/Pnoc09NTQXrR48eDdYTOgu125harVlsCvzMmTPBeuws5TLNz88H67HzXPfu3RusP3z4sKwlJYGp1d4Uy9ilS5eC9di5pmX2D52+akOnYucTHz58OFi/fv16Kc9bIqZWAQAAckMjBwAAkCgaOQAAgETRyAEAACSKRg4AACBRTK0CX8bUao/avHlzsH7s2LFgPXbm4po1a4L1K1euRJ/7xo0bwfqdO3einwOmVlMzODgYrO/ZsydY379/f7B+8ODBjp+7rKnVc+fOBesnT54M1mdnZzv6+jViahUAACA3NHIAAACJopEDAABIFI0cAABAomjkAAAAErXk1KqZbZR0SdKAJJd01t1/Z2a/knRU0vPiQ0+4+x+X+FpMBqHXdXVqlTyhSbo5tUqW0DDRn01fa+OTv5A05u7TZvYNSX81sw+K9/3G3U+VtUqgAcgTUA6yBKiNRs7dZyXNFo/nzOyepPXdXhiQI/IElIMsAS0d3SNnZpskbZe08CqYPzezv5vZBTN7NfI5o2Z218zurmilQGbIE1AOsoQma/tkBzPrk/RnSe+4+zUzG5D0qVr3Jvxa0qC7/2SJr8F9COh1lZzsQJ7QBFWc7ECW0BArO9nBzFZJuippwt2vSZK7P3X3/7n7vKRzkobLWi2QM/IElIMsAW00ctY6AG1c0j13P72ovvhQtgOSPip/eUBeyBNQDrIEtLQztfpdST+WNGNmfytqJyS9aWbfVuvX1/+W9NOurBDIC3kCykGWAHVwj1wpT8Z9COh9ldwjVwbyhF5XxT1yZSBLSMDK7pEDAABA76GRAwAASBSNHAAAQKJo5AAAABJFIwcAAJAoGjkAAIBE0cgBAAAkikYOAAAgUTRyAAAAiWrniK4yfSrpUfG4v3i7KdhvGr5Z9wI6QJ6aI8X9kqV0NG3PKe43mqdKj+j60hOb3U3lKKQysF90U9O+3+wX3dLE73XT9pzbfvnTKgAAQKJo5AAAABJVZyN3tsbnrgP7RTc17fvNftEtTfxeN23PWe23tnvkAAAAsDL8aRUAACBRNHIAAACJqryRM7N9ZnbfzD42s7erfv4qmNkFM3tmZh8tqq0xsw/M7EHx76t1rrFMZrbRzP5kZv80s3+Y2S+KerZ77hXkKb9rizzVhzzldW01JUuVNnJm9oqk30v6gaStkt40s61VrqEiFyXte6n2tqQP3f01SR8Wb+fiC0lj7r5V0nck/az475rznmtHnrK9tshTDchTltdWI7JU9W/khiV97O7/cvf/SrosaaTiNXSdu9+U9NlL5RFJ7xaP35X0w0oX1UXuPuvu08XjOUn3JK1XxnvuEeSpJatrizzVhjy1ZHNtNSVLVTdy6yU9XvT2k6LWBAPuPls8/kTSQJ2L6RYz2yRpu6Q7asiea0SeWrK9tshTpchTS5bXVs5ZYtihBt56zZfsXvfFzPokXZX0lrt/vvh9ue4Z9cv12iJPqEOO11buWaq6kfuPpI2L3t5Q1JrgqZkNSlLx77Oa11MqM1ulVlAm3P1aUc56zz2APCnPa4s81YI8Kb9rqwlZqrqR+4uk18zsW2b2dUk/kjRZ8RrqMinpSPH4iKT3a1xLqczMJI1Luufupxe9K9s99wjy1JLVtUWeakOeWrK5tpqSpcpPdjCz/ZJ+K+kVSRfc/Z1KF1ABM/uDpO9J6pf0VNIvJd2Q9J6kIUmPJL3h7i/fcJokM3td0pSkGUnzRfmEWvciZLnnXkGe8ru2yFN9yFNe11ZTssQRXQAAAIli2AEAACBRNHIAAACJopEDAABIFI0cAABAomjkAAAAEkUjBwAAkCgaOQAAgET9H3f6k73DvEf7AAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.rcParams['figure.figsize'] = (9,9) # Make the figures a bit bigger\n",
        "\n",
        "for i in range(9):\n",
        "    plt.subplot(3,3,i+1)\n",
        "    num = random.randint(0, len(X_train))\n",
        "    plt.imshow(X_train[num], cmap='gray', interpolation='none')\n",
        "    plt.title(\"Class {}\".format(y_train[num]))\n",
        "    \n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNr0c3ErEkTx"
      },
      "source": [
        "Let's examine a single digit a little closer, and print out the array representing the last digit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "8zMITsMNEkTx",
        "outputId": "a7817b52-50aa-4d39-db27-190c9152bcdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0  0  0  0  0   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   0  0  0  0  0  0  \n",
            "0  0  0  0  0   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   0  0  0  0  0  0  \n",
            "0  0  0  0  0   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   0  0  0  0  0  0  \n",
            "0  0  0  0  0   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   0  0  0  0  0  0  \n",
            "0  0  0  0  0   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   0  0  0  0  0  0  \n",
            "0  0  0  0  0   0  147  153   29  128  253  153  141  141  141   41    0    0    0    0    0    0   0  0  0  0  0  0  \n",
            "0  0  0  0  0  38  234  252  253  252  252  252  253  252  252  252  157   44    0    0    0    0   0  0  0  0  0  0  \n",
            "0  0  0  0  0  38  234  252  253  233  168  243  253  252  252  252  253  240   81    0    0    0   0  0  0  0  0  0  \n",
            "0  0  0  0  0   0   59  190   78   22    0   25   28  103  177  252  253  252  168    0    0    0   0  0  0  0  0  0  \n",
            "0  0  0  0  0   0    0    0    0    0    0    0    0    0    0  176  255  253  168    0    0    0   0  0  0  0  0  0  \n",
            "0  0  0  0  0   0    0    0    0    0    0    0   32   57  144  243  253  252  142    0    0    0   0  0  0  0  0  0  \n",
            "0  0  0  0  0   0    0    0    0    0   29  134  229  252  252  252  253  170   13    0    0    0   0  0  0  0  0  0  \n",
            "0  0  0  0  0   0    0    0    0   51  234  252  253  252  252  151   78    9    0    0    0    0   0  0  0  0  0  0  \n",
            "0  0  0  0  0   0    0    0   16  216  253  253  179   22    0    0    0    0    0    0    0    0   0  0  0  0  0  0  \n",
            "0  0  0  0  0   0    0    0   22  234  252  252  253  234  169   44    0    0    0    0    0    0   0  0  0  0  0  0  \n",
            "0  0  0  0  0   0    0    0    0   63  168  243  253  252  252  228  135   28    0    0    0    0   0  0  0  0  0  0  \n",
            "0  0  0  0  0   0    0    0    0    0    0   25   91  215  252  252  253  234  100    0    0    0   0  0  0  0  0  0  \n",
            "0  0  0  0  0   0    0    0    0    0    0    0    0    0   89  226  254  253  253  153   13    0   0  0  0  0  0  0  \n",
            "0  0  0  0  0   0    0    0    0    0    0    0    0    0    0   38  146  234  252  252  138    0   0  0  0  0  0  0  \n",
            "0  0  0  0  0   0    0    0    0    0    0    0    0    0    0    0    0   63  234  252  247  104   0  0  0  0  0  0  \n",
            "0  0  0  0  0   0    0    0    0    0    0    0    0    0    0    0    0    0   59  240  253  221  25  0  0  0  0  0  \n",
            "0  0  0  0  0   0    0    0    0   32  141   91   13    0    0    0    0    0   10  229  254  253  56  0  0  0  0  0  \n",
            "0  0  0  0  0   0    0    0    0   44  240  252  207  169   82  157  169  169  197  252  253  214  19  0  0  0  0  0  \n",
            "0  0  0  0  0   0    0    0    0    0   81  243  253  252  252  252  253  252  252  252  244   81   0  0  0  0  0  0  \n",
            "0  0  0  0  0   0    0    0    0    0    0   25   91  165  252  252  253  252  252  151   25    0   0  0  0  0  0  0  \n",
            "0  0  0  0  0   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   0  0  0  0  0  0  \n",
            "0  0  0  0  0   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   0  0  0  0  0  0  \n",
            "0  0  0  0  0   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   0  0  0  0  0  0  \n"
          ]
        }
      ],
      "source": [
        "# just a little function for pretty printing a matrix\n",
        "def matprint(mat, fmt=\"g\"):\n",
        "    col_maxes = [max([len((\"{:\"+fmt+\"}\").format(x)) for x in col]) for col in mat.T]\n",
        "    for x in mat:\n",
        "        for i, y in enumerate(x):\n",
        "            print((\"{:\"+str(col_maxes[i])+fmt+\"}\").format(y), end=\"  \")\n",
        "        print(\"\")\n",
        "\n",
        "# now print!        \n",
        "matprint(X_train[num])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx24ATqYEkTy"
      },
      "source": [
        "Each pixel is an 8-bit integer from 0-255. 0 is full black, while 255 is full white. This what we call a single-channel pixel. It's called monochrome.\n",
        "\n",
        "*Fun-fact! Your computer screen has three channels for each pixel: red, green, blue. Each of these channels also likely takes an 8-bit integer. 3 channels -- 24 bits total -- 16,777,216 possible colors!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dkhDg2IEkTy"
      },
      "source": [
        "## Formatting the input data layer\n",
        "\n",
        "Instead of a 28 x 28 matrix, we build our network to accept a 784-length vector.\n",
        "\n",
        "Each image needs to be then reshaped (or flattened) into a vector. We'll also normalize the inputs to be in the range [0-1] rather than [0-255]. Normalizing inputs is generally recommended, so that any additional dimensions (for other network architectures) are of the same scale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jxm_90TpEkTz"
      },
      "source": [
        "<img src='https://github.com/jcanoj/UNED/blob/master/keras_mnist_tutorial/flatten.png?raw=1' >"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "lvbIowiwEkTz",
        "outputId": "9f7c348f-c851-49fe-9100-e2f56177e223",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training matrix shape (60000, 784)\n",
            "Testing matrix shape (10000, 784)\n"
          ]
        }
      ],
      "source": [
        "X_train = X_train.reshape(60000, 784) # reshape 60,000 28 x 28 matrices into 60,000 784-length vectors.\n",
        "X_test = X_test.reshape(10000, 784)   # reshape 10,000 28 x 28 matrices into 10,000 784-length vectors.\n",
        "\n",
        "X_train = X_train.astype('float32')   # change integers to 32-bit floating point numbers\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "X_train /= 255                        # normalize each value for each pixel for the entire vector for each input\n",
        "X_test /= 255\n",
        "\n",
        "print(\"Training matrix shape\", X_train.shape)\n",
        "print(\"Testing matrix shape\", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZO-1S9UEkT0"
      },
      "source": [
        "We then modify our classes (unique digits) to be in the one-hot format, i.e.\n",
        "\n",
        "```\n",
        "0 -> [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "1 -> [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
        "2 -> [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
        "etc.\n",
        "```\n",
        "\n",
        "If the final output of our network is very close to one of these classes, then it is most likely that class. For example, if the final output is:\n",
        "\n",
        "```\n",
        "[0, 0.94, 0, 0, 0, 0, 0.06, 0, 0]\n",
        "```\n",
        "then it is most probable that the image is that of the digit `1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "collapsed": true,
        "id": "Gc8SZJFtEkT0"
      },
      "outputs": [],
      "source": [
        "nb_classes = 10 # number of unique digits\n",
        "\n",
        "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, nb_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JWh1frEEkT0"
      },
      "source": [
        "# Building a 3-layer fully connected network (FCN)\n",
        "\n",
        "<img src=\"https://github.com/jcanoj/UNED/blob/master/keras_mnist_tutorial/figure.png?raw=1\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "collapsed": true,
        "id": "VxU6XwIxEkT1"
      },
      "outputs": [],
      "source": [
        "# The Sequential model is a linear stack of layers and is very common.\n",
        "\n",
        "model = Sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5m1g33zfEkT1"
      },
      "source": [
        "## The first hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "collapsed": true,
        "id": "0-3iL0CEEkT1"
      },
      "outputs": [],
      "source": [
        "# The first hidden layer is a set of 512 nodes (artificial neurons).\n",
        "# Each node will receive an element from each input vector and apply some weight and bias to it.\n",
        "\n",
        "model.add(Dense(512, input_shape=(784,))) #(784,) is not a typo -- that represents a 784 length vector!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "collapsed": true,
        "id": "nlxQUh1REkT2"
      },
      "outputs": [],
      "source": [
        "# An \"activation\" is a non-linear function applied to the output of the layer above.\n",
        "# It checks the new value of the node, and decides whether that artifical neuron has fired.\n",
        "# The Rectified Linear Unit (ReLU) converts all negative inputs to nodes in the next layer to be zero.\n",
        "# Those inputs are then not considered to be fired.\n",
        "# Positive values of a node are unchanged.\n",
        "\n",
        "model.add(Activation('relu'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5GK-DgyEkT2"
      },
      "source": [
        "$$f(x) = max (0,x)$$\n",
        "<img src = 'relu.jpg' >"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "collapsed": true,
        "id": "dTO_Q50FEkT3"
      },
      "outputs": [],
      "source": [
        "# Dropout zeroes a selection of random outputs (i.e., disables their activation)\n",
        "# Dropout helps protect the model from memorizing or \"overfitting\" the training data.\n",
        "model.add(Dropout(0.2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boAHKVqYEkT3"
      },
      "source": [
        "## Adding the second hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "collapsed": true,
        "id": "sC8LWjuVEkT3"
      },
      "outputs": [],
      "source": [
        "# The second hidden layer appears identical to our first layer.\n",
        "# However, instead of each of the 512-node receiving 784-inputs from the input image data,\n",
        "# they receive 512 inputs from the output of the first 512-node layer.\n",
        "\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjEv84b5EkT3"
      },
      "source": [
        "## The Final Output Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "collapsed": true,
        "id": "88X1tBH9EkT4"
      },
      "outputs": [],
      "source": [
        "# The final layer of 10 neurons in fully-connected to the previous 512-node layer.\n",
        "# The final layer of a FCN should be equal to the number of desired classes (10 in this case).\n",
        "model.add(Dense(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "collapsed": true,
        "id": "OxIgQm0iEkT4"
      },
      "outputs": [],
      "source": [
        "# The \"softmax\" activation represents a probability distribution over K different possible outcomes.\n",
        "# Its values are all non-negative and sum to 1.\n",
        "\n",
        "model.add(Activation('softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "3e94wHVlEkT4",
        "outputId": "e913ea70-0a15-4d59-f8bf-3b05f02f3c59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_6 (Dense)             (None, 512)               401920    \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 512)               0         \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 512)               262656    \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 512)               0         \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 669,706\n",
            "Trainable params: 669,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Summarize the built model\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQavAKJYEkT4"
      },
      "source": [
        "## Compiling the model\n",
        "\n",
        "Keras is built on top of Theano and TensorFlow. Both packages allow you to define a *computation graph* in Python, which then compiles and runs efficiently on the CPU or GPU without the overhead of the Python interpreter.\n",
        "\n",
        "When compiing a model, Keras asks you to specify your **loss function** and your **optimizer**. The loss function we'll use here is called *categorical cross-entropy*, and is a loss function well-suited to comparing two probability distributions.\n",
        "\n",
        "Our predictions are probability distributions across the ten different digits (e.g. \"we're 80% confident this image is a 3, 10% sure it's an 8, 5% it's a 2, etc.\"), and the target is a probability distribution with 100% for the correct category, and 0 for everything else. The cross-entropy is a measure of how different your predicted distribution is from the target distribution. [More detail at Wikipedia](https://en.wikipedia.org/wiki/Cross_entropy)\n",
        "\n",
        "The optimizer helps determine how quickly the model learns through **gradient descent**. The rate at which descends a gradient is called the **learning rate**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4U8RsM4dEkT5"
      },
      "source": [
        "<img src = \"gradient_descent.png\" >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzy3X7MmEkT5"
      },
      "source": [
        "<img src = \"learning_rate.png\" >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYGO0nVQEkT5"
      },
      "source": [
        "So are smaller learning rates better? Not quite! It's important for an optimizer not to get stuck in local minima while neglecting the global minimum of the loss function. Sometimes that means trying a larger learning rate to jump out of a local minimum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ez3CWEIjEkT5"
      },
      "source": [
        "<img src = 'complicated_loss_function.png' >"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "collapsed": true,
        "id": "tqar9sMbEkT6"
      },
      "outputs": [],
      "source": [
        "# Let's use the Adam optimizer for learning\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XH_TF3LEkT6"
      },
      "source": [
        "## Train the model!\n",
        "This is the fun part! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvH1SEupEkT6"
      },
      "source": [
        "The **batch size** determines over **how much data per step is used to compute the loss function, gradients, and back propagation**. Large batch sizes allow the network to complete it's training faster; however, there are other factors beyond training speed to consider.\n",
        "\n",
        "- Too large of a batch size smoothes the local minima of the loss function, causing the optimizer to settle in one because it thinks it found the global minimum.\n",
        "\n",
        "- Too small of a batch size creates a very noisy loss function, and the optimizer may never find the global minimum.\n",
        "\n",
        "So a good batch size may take some trial and error to find!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "hsRPltEvEkT6",
        "outputId": "b25a6a82-3833-448d-9e94-c699f198dcb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "469/469 [==============================] - 10s 19ms/step - loss: 0.2484 - accuracy: 0.9251\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 7s 16ms/step - loss: 0.1030 - accuracy: 0.9683\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 7s 16ms/step - loss: 0.0724 - accuracy: 0.9775\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 10s 20ms/step - loss: 0.0539 - accuracy: 0.9828\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 10s 21ms/step - loss: 0.0452 - accuracy: 0.9848\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa1d403a310>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "model.fit(X_train, Y_train,\n",
        "          batch_size=128, epochs=5,\n",
        "          verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R-y9rmGEkT6"
      },
      "source": [
        "The two numbers, in order, represent the value of the loss function of the network on the training set, and the overall accuracy of the network on the training data. But how does it do on data it did not train on?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSmhO1gkEkT7"
      },
      "source": [
        "## Evaluate Model's Accuracy on Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "0nx5FJJkEkT7",
        "outputId": "63dfa56a-7797-4000-8dcf-fb0aa05c4194",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0678 - accuracy: 0.9799\n",
            "Test score: 0.06780592352151871\n",
            "Test accuracy: 0.9799000024795532\n"
          ]
        }
      ],
      "source": [
        "score = model.evaluate(X_test, Y_test)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImenjcfKEkT7"
      },
      "source": [
        "### Inspecting the output\n",
        "\n",
        "It's always a good idea to inspect the output and make sure everything looks sane. Here we'll look at some examples it gets right, and some examples it gets wrong."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "yD_sFDIYEkT7"
      },
      "outputs": [],
      "source": [
        "# The predict_classes function outputs the highest probability class\n",
        "# according to the trained classifier for each input example.\n",
        "predicted_classes = model.predict_classes(X_test)\n",
        "\n",
        "# Check which items we got right / wrong\n",
        "correct_indices = np.nonzero(predicted_classes == y_test)[0]\n",
        "\n",
        "incorrect_indices = np.nonzero(predicted_classes != y_test)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diP3B3CiEkT7"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "for i, correct in enumerate(correct_indices[:9]):\n",
        "    plt.subplot(3,3,i+1)\n",
        "    plt.imshow(X_test[correct].reshape(28,28), cmap='gray', interpolation='none')\n",
        "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[correct], y_test[correct]))\n",
        "    \n",
        "plt.tight_layout()\n",
        "    \n",
        "plt.figure()\n",
        "for i, incorrect in enumerate(incorrect_indices[:9]):\n",
        "    plt.subplot(3,3,i+1)\n",
        "    plt.imshow(X_test[incorrect].reshape(28,28), cmap='gray', interpolation='none')\n",
        "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[incorrect], y_test[incorrect]))\n",
        "    \n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjHoPFGpEkT8"
      },
      "source": [
        "# Trying experimenting with the batch size!\n",
        "\n",
        "#### How does increasing the batch size to 10,000 affect the training time and test accuracy?\n",
        "\n",
        "#### How about a batch size of 32?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBd--0yXEkT8"
      },
      "source": [
        "# Introducing Convolution! What is it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oki_C2ZEkT9"
      },
      "source": [
        "Before, we built a network that accepts the normalized pixel values of each value and operates soley on those values. What if we could instead feed different features (e.g. **curvature, edges**) of each image into a network, and have the network learn which features are important for classifying an image?\n",
        "\n",
        "This possible through convolution! Convolution applies **kernels** (filters) that traverse through each image and generate **feature maps**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WfBM5sfEkT9"
      },
      "source": [
        "<img src = 'convolution.gif' >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgf9G4RyEkT9"
      },
      "source": [
        "In the above example, the image is a 5 x 5 matrix and the kernel going over it is a 3 x 3 matrix. A dot product operation takes place between the image and the kernel and the convolved feature is generated. Each kernel in a CNN learns a different characteristic of an image.\n",
        "\n",
        "Kernels are often used in photoediting software to apply blurring, edge detection, sharpening, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6id-KAnEkT-"
      },
      "source": [
        "<img src = 'kernels.png' >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7BHj-2kEkT-"
      },
      "source": [
        "Kernels in deep learning networks are used in similar ways, i.e. highlighting some feature. Combined with a system called **max pooling**, the non-highlighted elements are discarded from each feature map, leaving only the features of interest, reducing the number of learned parameters, and decreasing the computational cost (e.g. system memory)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zDXmveHEkT_"
      },
      "source": [
        "<img src = 'max_pooling.png' >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bnd6CLifEkT_"
      },
      "source": [
        "We can also take convolutions of convolutions -- we can stack as many convolutions as we want, as long as there are enough pixels to fit a kernel.\n",
        "\n",
        "*Warning: What you may find down there in those deep convolutions may not appear recognizable to you.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VRtlyHiEkT_"
      },
      "source": [
        "<img src = 'go_deeper.jpg' >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar8XSawmEkUA"
      },
      "source": [
        "## Building a \"Deep\" Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "collapsed": true,
        "id": "9WBbPkIaEkUA"
      },
      "outputs": [],
      "source": [
        "# import some additional tools\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, Flatten\n",
        "\n",
        "# from keras.layers.normalization import BatchNormalization # deprecated\n",
        "from tensorflow.keras.layers import BatchNormalization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "collapsed": true,
        "id": "JaZ_SZTUEkUA"
      },
      "outputs": [],
      "source": [
        "# Reload the MNIST data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "QryTjXCaEkUA",
        "outputId": "74c22125-6588-4d16-b65e-9478abd0de2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training matrix shape (60000, 28, 28, 1)\n",
            "Testing matrix shape (10000, 28, 28, 1)\n"
          ]
        }
      ],
      "source": [
        "# Again, do some formatting\n",
        "# Except we do not flatten each image into a 784-length vector because we want to perform convolutions first\n",
        "\n",
        "X_train = X_train.reshape(60000, 28, 28, 1) #add an additional dimension to represent the single-channel\n",
        "X_test = X_test.reshape(10000, 28, 28, 1)\n",
        "\n",
        "X_train = X_train.astype('float32')         # change integers to 32-bit floating point numbers\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "X_train /= 255                              # normalize each value for each pixel for the entire vector for each input\n",
        "X_test /= 255\n",
        "\n",
        "print(\"Training matrix shape\", X_train.shape)\n",
        "print(\"Testing matrix shape\", X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "collapsed": true,
        "id": "9_ONWKXeEkUB"
      },
      "outputs": [],
      "source": [
        "# one-hot format classes\n",
        "\n",
        "nb_classes = 10 # number of unique digits\n",
        "\n",
        "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, nb_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "collapsed": true,
        "id": "5kte5U2rEkUB"
      },
      "outputs": [],
      "source": [
        "model = Sequential()                                 # Linear stacking of layers\n",
        "\n",
        "# Convolution Layer 1\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(28,28,1))) # 32 different 3x3 kernels -- so 32 feature maps\n",
        "model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation\n",
        "convLayer01 = Activation('relu')                     # activation\n",
        "model.add(convLayer01)\n",
        "\n",
        "# Convolution Layer 2\n",
        "model.add(Conv2D(32, (3, 3)))                        # 32 different 3x3 kernels -- so 32 feature maps\n",
        "model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation\n",
        "model.add(Activation('relu'))                        # activation\n",
        "convLayer02 = MaxPooling2D(pool_size=(2,2))          # Pool the max values over a 2x2 kernel\n",
        "model.add(convLayer02)\n",
        "\n",
        "# Convolution Layer 3\n",
        "model.add(Conv2D(64,(3, 3)))                         # 64 different 3x3 kernels -- so 64 feature maps\n",
        "model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation\n",
        "convLayer03 = Activation('relu')                     # activation\n",
        "model.add(convLayer03)\n",
        "\n",
        "# Convolution Layer 4\n",
        "model.add(Conv2D(64, (3, 3)))                        # 64 different 3x3 kernels -- so 64 feature maps\n",
        "model.add(BatchNormalization(axis=-1))               # normalize each feature map before activation\n",
        "model.add(Activation('relu'))                        # activation\n",
        "convLayer04 = MaxPooling2D(pool_size=(2,2))          # Pool the max values over a 2x2 kernel\n",
        "model.add(convLayer04)\n",
        "model.add(Flatten())                                 # Flatten final 4x4x64 output matrix into a 1024-length vector\n",
        "\n",
        "# Fully Connected Layer 5\n",
        "model.add(Dense(512))                                # 512 FCN nodes\n",
        "model.add(BatchNormalization())                      # normalization\n",
        "model.add(Activation('relu'))                        # activation\n",
        "\n",
        "# Fully Connected Layer 6                       \n",
        "model.add(Dropout(0.2))                              # 20% dropout of randomly selected nodes\n",
        "model.add(Dense(10))                                 # final 10 FCN nodes\n",
        "model.add(Activation('softmax'))                     # softmax activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "ZVoEApFMEkUC",
        "outputId": "a0258829-997b-4738-a8ff-54313b210fa8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 26, 26, 32)       128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 26, 26, 32)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 24, 24, 32)        9248      \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 24, 24, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 24, 24, 32)        0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 12, 12, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 10, 10, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 10, 10, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 10, 10, 64)        0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 8, 8, 64)          36928     \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 8, 8, 64)         256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 4, 4, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 512)               524800    \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 512)              2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_12 (Activation)  (None, 512)               0         \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 10)                5130      \n",
            "                                                                 \n",
            " activation_13 (Activation)  (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 597,738\n",
            "Trainable params: 596,330\n",
            "Non-trainable params: 1,408\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "collapsed": true,
        "id": "QMT7d4fsEkUC"
      },
      "outputs": [],
      "source": [
        "# we'll use the same optimizer\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "collapsed": true,
        "id": "AotX75kYEkUC"
      },
      "outputs": [],
      "source": [
        "# data augmentation prevents overfitting by slightly changing the data randomly\n",
        "# Keras has a great built-in feature to do automatic augmentation\n",
        "\n",
        "gen = ImageDataGenerator(rotation_range=8, width_shift_range=0.08, shear_range=0.3,\n",
        "                         height_shift_range=0.08, zoom_range=0.08)\n",
        "\n",
        "test_gen = ImageDataGenerator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "collapsed": true,
        "id": "Z9qgazymEkUD"
      },
      "outputs": [],
      "source": [
        "# We can then feed our augmented data in batches\n",
        "# Besides loss function considerations as before, this method actually results in significant memory savings\n",
        "# because we are actually LOADING the data into the network in batches before processing each batch\n",
        "\n",
        "# Before the data was all loaded into memory, but then processed in batches.\n",
        "\n",
        "train_generator = gen.flow(X_train, Y_train, batch_size=128)\n",
        "test_generator = test_gen.flow(X_test, Y_test, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "jzXGpom-EkUD",
        "outputId": "b0634bd3-826d-49ac-fca2-794fd2941ea5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  import sys\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "468/468 [==============================] - 189s 401ms/step - loss: 0.1311 - accuracy: 0.9591 - val_loss: 1.0827 - val_accuracy: 0.6599\n",
            "Epoch 2/5\n",
            "468/468 [==============================] - 191s 409ms/step - loss: 0.0506 - accuracy: 0.9838 - val_loss: 0.0346 - val_accuracy: 0.9898\n",
            "Epoch 3/5\n",
            "468/468 [==============================] - 187s 399ms/step - loss: 0.0394 - accuracy: 0.9874 - val_loss: 0.0440 - val_accuracy: 0.9859\n",
            "Epoch 4/5\n",
            "468/468 [==============================] - 189s 404ms/step - loss: 0.0324 - accuracy: 0.9897 - val_loss: 0.0719 - val_accuracy: 0.9788\n",
            "Epoch 5/5\n",
            "468/468 [==============================] - 189s 403ms/step - loss: 0.0314 - accuracy: 0.9903 - val_loss: 0.0662 - val_accuracy: 0.9781\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa1db1a1dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "# We can now train our model which is fed data by our batch loader\n",
        "# Steps per epoch should always be total size of the set divided by the batch size\n",
        "\n",
        "# SIGNIFICANT MEMORY SAVINGS (important for larger, deeper networks)\n",
        "\n",
        "model.fit_generator(train_generator, steps_per_epoch=60000//128, epochs=5, verbose=1, \n",
        "                    validation_data=test_generator, validation_steps=10000//128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "2E0ArtbMEkUD",
        "outputId": "8661b81d-a4a4-4a6d-d813-7f814a8fb122",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 10s 31ms/step - loss: 0.0661 - accuracy: 0.9781\n",
            "Test score: 0.06609976291656494\n",
            "Test accuracy: 0.9781000018119812\n"
          ]
        }
      ],
      "source": [
        "score = model.evaluate(X_test, Y_test)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqdD2_MlEkUD"
      },
      "source": [
        "## Great results! \n",
        "\n",
        "But wouldn't it be nice if we could visualize those convolutions so that we can see what the model is seeing?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "collapsed": true,
        "id": "tcNIBAvBEkUE"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "\n",
        "# choose any image to want by specifying the index\n",
        "img = X_test[3]\n",
        "img = np.expand_dims(img, axis=0) # Keras requires the image to be in 4D, so we add an extra dimension to it.\n",
        "\n",
        "# Not important to understand how this function work -- It just plots a convolution layer\n",
        "\n",
        "def visualize(layer):\n",
        "    inputs = [K.learning_phase()] + model.inputs\n",
        "    \n",
        "    _convout1_f = K.function(inputs, [layer.output])\n",
        "    \n",
        "    def convout1_f(X):\n",
        "        # The [0] is to disable the training phase flag\n",
        "        return _convout1_f([0] + [X])\n",
        "\n",
        "    convolutions = convout1_f(img)\n",
        "    convolutions = np.squeeze(convolutions)\n",
        "\n",
        "    print ('Shape of conv:', convolutions.shape)\n",
        "    \n",
        "    m = convolutions.shape[2]\n",
        "    n = int(np.ceil(np.sqrt(m)))\n",
        "    \n",
        "    # Visualization of each filter of the layer\n",
        "    fig = plt.figure(figsize=(15,12))\n",
        "    for i in range(m):\n",
        "        ax = fig.add_subplot(n,n,i+1)\n",
        "        ax.imshow(convolutions[:,:,i], cmap='gray')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "b96i1SGSEkUE",
        "outputId": "0e9a944b-aa31-4221-cd8d-f5606c54a75c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa1d4324b10>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x648 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure()\n",
        "plt.imshow(X_test[3].reshape(28,28), cmap='gray', interpolation='none')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "8yHgQrf_EkUE",
        "outputId": "f9758618-c855-4dd9-c740-198f239c12d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-803db138f5d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvLayer01\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# visualize first set of feature maps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-57-454b785c5aae>\u001b[0m in \u001b[0;36mvisualize\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0m_convout1_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvout1_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/backend.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, updates, name, **kwargs)\u001b[0m\n\u001b[1;32m   4316\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4317\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_utils\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4318\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4320\u001b[0m     \u001b[0mwrap_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m       if not all([functional_utils.is_input_keras_tensor(t)\n\u001b[0;32m--> 144\u001b[0;31m                   for t in tf.nest.flatten(inputs)]):\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctional_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone_graph_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m       if not all([functional_utils.is_input_keras_tensor(t)\n\u001b[0;32m--> 144\u001b[0;31m                   for t in tf.nest.flatten(inputs)]):\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctional_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone_graph_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional_utils.py\u001b[0m in \u001b[0;36mis_input_keras_tensor\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m     45\u001b[0m   \"\"\"\n\u001b[1;32m     46\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnode_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_keras_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_KERAS_TENSOR_TYPE_CHECK_ERROR_MSG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found unexpected instance while processing input tensors for keras functional model. Expecting KerasTensor which is from tf.keras.Input() or output from keras layer call(). Got: 0"
          ]
        }
      ],
      "source": [
        "visualize(convLayer01) # visualize first set of feature maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "ikJTtpI4EkUE",
        "outputId": "64d59d26-d689-4d33-bca7-53024496c01b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-bf2a2a446b0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvLayer02\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# visualize second set of feature maps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-57-454b785c5aae>\u001b[0m in \u001b[0;36mvisualize\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0m_convout1_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvout1_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/backend.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, updates, name, **kwargs)\u001b[0m\n\u001b[1;32m   4316\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4317\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_utils\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4318\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4320\u001b[0m     \u001b[0mwrap_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m       if not all([functional_utils.is_input_keras_tensor(t)\n\u001b[0;32m--> 144\u001b[0;31m                   for t in tf.nest.flatten(inputs)]):\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctional_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone_graph_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m       if not all([functional_utils.is_input_keras_tensor(t)\n\u001b[0;32m--> 144\u001b[0;31m                   for t in tf.nest.flatten(inputs)]):\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctional_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone_graph_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional_utils.py\u001b[0m in \u001b[0;36mis_input_keras_tensor\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m     45\u001b[0m   \"\"\"\n\u001b[1;32m     46\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnode_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_keras_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_KERAS_TENSOR_TYPE_CHECK_ERROR_MSG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found unexpected instance while processing input tensors for keras functional model. Expecting KerasTensor which is from tf.keras.Input() or output from keras layer call(). Got: 0"
          ]
        }
      ],
      "source": [
        "visualize(convLayer02) # visualize second set of feature maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "xhx6HPn0EkUF",
        "outputId": "31a4bb79-2a04-454e-c592-615b3226e8ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-5e9aa6ad8b88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvLayer03\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# visualize third set of feature maps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-57-454b785c5aae>\u001b[0m in \u001b[0;36mvisualize\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0m_convout1_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvout1_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/backend.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, updates, name, **kwargs)\u001b[0m\n\u001b[1;32m   4316\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4317\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_utils\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4318\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4320\u001b[0m     \u001b[0mwrap_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m       if not all([functional_utils.is_input_keras_tensor(t)\n\u001b[0;32m--> 144\u001b[0;31m                   for t in tf.nest.flatten(inputs)]):\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctional_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone_graph_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m       if not all([functional_utils.is_input_keras_tensor(t)\n\u001b[0;32m--> 144\u001b[0;31m                   for t in tf.nest.flatten(inputs)]):\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctional_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone_graph_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional_utils.py\u001b[0m in \u001b[0;36mis_input_keras_tensor\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m     45\u001b[0m   \"\"\"\n\u001b[1;32m     46\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnode_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_keras_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_KERAS_TENSOR_TYPE_CHECK_ERROR_MSG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found unexpected instance while processing input tensors for keras functional model. Expecting KerasTensor which is from tf.keras.Input() or output from keras layer call(). Got: 0"
          ]
        }
      ],
      "source": [
        "visualize(convLayer03)# visualize third set of feature maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "yKGbtvzuEkUF",
        "outputId": "0fbcd016-af33-46d7-dc16-4b248cb21aed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-b94e8d26c675>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvLayer04\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# visualize fourth set of feature maps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-57-454b785c5aae>\u001b[0m in \u001b[0;36mvisualize\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0m_convout1_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvout1_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/backend.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, updates, name, **kwargs)\u001b[0m\n\u001b[1;32m   4316\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4317\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_utils\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4318\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4320\u001b[0m     \u001b[0mwrap_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m       if not all([functional_utils.is_input_keras_tensor(t)\n\u001b[0;32m--> 144\u001b[0;31m                   for t in tf.nest.flatten(inputs)]):\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctional_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone_graph_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m       if not all([functional_utils.is_input_keras_tensor(t)\n\u001b[0;32m--> 144\u001b[0;31m                   for t in tf.nest.flatten(inputs)]):\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctional_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone_graph_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional_utils.py\u001b[0m in \u001b[0;36mis_input_keras_tensor\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m     45\u001b[0m   \"\"\"\n\u001b[1;32m     46\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnode_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_keras_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_KERAS_TENSOR_TYPE_CHECK_ERROR_MSG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found unexpected instance while processing input tensors for keras functional model. Expecting KerasTensor which is from tf.keras.Input() or output from keras layer call(). Got: 0"
          ]
        }
      ],
      "source": [
        "visualize(convLayer04)# visualize fourth set of feature maps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLhdDY63EkUF"
      },
      "source": [
        "#### For a 3D visualization of a very similar network, visit http://scs.ryerson.ca/~aharley/vis/conv/"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Tensorflow (GPU)",
      "language": "python",
      "name": "py3.6-tfgpu"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "MNIST in Keras.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}